#!/usr/bin/env python3
"""Thin wrapper that forwards to the official Codex CLI binary.

The global `@openai/codex` install drops a `codex` executable in `/usr/local/bin`.
This wrapper preserves developer conveniences that AstraForge relied upon in the
Python stub (for example `codex --spec foo.json`) while delegating the actual work
to the upstream CLI. It also guarantees a local `~/.codex/config.toml` exists so
the CLI can talk to the workspace-scoped LLM proxy without exposing the real API
key to the container.
"""

from __future__ import annotations

import json
import os
import shlex
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Iterable, Sequence
from urllib.parse import urlparse, urlunparse

_KNOWN_SUBCOMMANDS = {"exec", "login", "logout", "version", "resume"}
_REAL_CLI_ENV = "CODEX_REAL_CLI"
_DEFAULT_REAL_CLI = "/usr/local/bin/codex-real"
_CONFIG_DIR = Path.home() / ".codex"
_CONFIG_PATH = _CONFIG_DIR / "config.toml"
_DEFAULT_PROXY_ENV = "CODEX_WRAPPER_DEFAULT_PROXY_URL"
_DEFAULT_API_KEY_ENV = "CODEX_WRAPPER_API_KEY"
_DEFAULT_PROXY = (
    os.getenv(_DEFAULT_PROXY_ENV)
    or os.getenv("LLM_PROXY_URL")
    or "http://localhost:5200"
)
_DEFAULT_MODEL = os.getenv("CODEX_WRAPPER_DEFAULT_MODEL", "gpt-5-codex")
_DEFAULT_PROVIDER_KEY = os.getenv(
    "CODEX_WRAPPER_MODEL_PROVIDER_KEY", "astraforge-proxy"
)
_DEFAULT_PROVIDER_NAME = os.getenv(
    "CODEX_WRAPPER_MODEL_PROVIDER_NAME", "AstraForge Proxy"
)
_DEFAULT_PROVIDER_WIRE_API = os.getenv(
    "CODEX_WRAPPER_MODEL_PROVIDER_WIRE_API", "responses"
)
_DEFAULT_PROVIDER_ENV_KEY = os.getenv(
    "CODEX_WRAPPER_PROVIDER_ENV_KEY", "ASTRAFORGE_PROXY_API_KEY"
)
_DEFAULT_IS_REASONING = (
    os.getenv("CODEX_WRAPPER_MODEL_IS_REASONING", "true").lower() == "true"
)
_DEFAULT_REASONING_EFFORT = os.getenv("CODEX_WRAPPER_MODEL_REASONING_EFFORT", "high")
_DEFAULT_PROFILE_NAME = "full_auto"


def _resolve_llm_provider() -> str:
    return (os.getenv("LLM_PROVIDER") or "").strip().lower()


def _resolve_default_profile() -> str:
    override = os.getenv("CODEX_WRAPPER_DEFAULT_PROFILE")
    if override:
        return override
    if _resolve_llm_provider() == "ollama":
        return "ollama"
    return _DEFAULT_PROFILE_NAME


def _format_provider_base_url(provider_key: str, proxy_url: str) -> str:
    raw_proxy = proxy_url.strip()
    if not raw_proxy:
        return proxy_url
    parsed = urlparse(raw_proxy)
    path = parsed.path.rstrip("/")
    provider_segment = provider_key
    if provider_key == "local-llm":
        provider_segment = "ollama"
    if provider_segment in {"openai", "ollama"}:
        if "/providers/" not in path:
            suffix = f"/providers/{provider_segment}"
            path = f"{path}{suffix}" if path else suffix
    if provider_segment == "ollama" and not path.endswith("/v1"):
        path = f"{path}/v1"
    base_url = urlunparse(parsed._replace(path=path))
    return base_url or proxy_url


def _normalize_proxy_url(proxy_url: str) -> str:
    trimmed = proxy_url.strip().rstrip("/")
    if trimmed.endswith("/v1"):
        trimmed = trimmed[:-3].rstrip("/")
    return trimmed or proxy_url


def _resolve_provider_config() -> tuple[str, str, str | None, str | None, str, str | None]:
    provider_key = _DEFAULT_PROVIDER_KEY
    provider_name = _DEFAULT_PROVIDER_NAME
    provider_wire_api = _DEFAULT_PROVIDER_WIRE_API
    provider_env_key = _DEFAULT_PROVIDER_ENV_KEY
    provider_context_window = None
    model = _DEFAULT_MODEL

    llm_provider = _resolve_llm_provider()
    if os.getenv("CODEX_WRAPPER_MODEL_PROVIDER_KEY") is None and llm_provider in {"openai", "ollama"}:
        provider_key = "local-llm" if llm_provider == "ollama" else llm_provider
        if os.getenv("CODEX_WRAPPER_MODEL_PROVIDER_NAME") is None:
            provider_name = "OpenAI" if llm_provider == "openai" else "Ollama"
        if os.getenv("CODEX_WRAPPER_PROVIDER_ENV_KEY") is None:
            provider_env_key = "OPENAI_API_KEY" if llm_provider == "openai" else "OLLAMA_API_KEY"
        if os.getenv("CODEX_WRAPPER_MODEL_PROVIDER_WIRE_API") is None:
            provider_wire_api = "responses" if llm_provider == "openai" else None
        if llm_provider == "ollama":
            ollama_context_window = os.getenv("OLLAMA_CONTEXT_WINDOW")
            if ollama_context_window:
                provider_context_window = ollama_context_window
            elif provider_key == "local-llm":
                provider_context_window = "16384"
        if llm_provider == "ollama":
            model = os.getenv("OLLAMA_MODEL") or model

    return (
        provider_key,
        provider_name,
        provider_wire_api,
        provider_env_key,
        model,
        provider_context_window,
    )

def _normalize_argv(argv: Sequence[str]) -> list[str]:
    if not argv:
        return list(argv)
    head = argv[0]
    if head in _KNOWN_SUBCOMMANDS:
        return list(argv)
    if head.startswith("-"):
        return ["exec", *argv]
    return list(argv)


def _convert_spec_flags(args: list[str]) -> list[str]:
    processed: list[str] = []
    i = 0
    while i < len(args):
        token = args[i]
        if token == "--spec":
            if i + 1 >= len(args):
                print(
                    "[codex-wrapper] Missing value for --spec flag.",
                    file=sys.stderr,
                    flush=True,
                )
                sys.exit(2)
            spec_value = args[i + 1]
            processed.extend(["-c", f"workspace.spec_path={json.dumps(spec_value)}"])
            i += 2
            continue
        if token.startswith("--spec="):
            _, spec_value = token.split("=", 1)
            processed.extend(["-c", f"workspace.spec_path={json.dumps(spec_value)}"])
            i += 1
            continue
        processed.append(token)
        i += 1
    return processed


def _has_profile_flag(args: Sequence[str]) -> bool:
    for token in args:
        if token in {"--profile", "-p"}:
            return True
        if token.startswith("--profile="):
            return True
    return False


def _extract_proxy_override(args: Iterable[str]) -> str | None:
    args_list = list(args)
    i = 0
    while i < len(args_list):
        token = args_list[i]
        if token in {"-c", "--config"} and i + 1 < len(args_list):
            candidate = args_list[i + 1]
            if "workspace.proxy_url=" in candidate:
                _, value = candidate.split("=", 1)
                try:
                    return json.loads(value)
                except json.JSONDecodeError:
                    return value
        i += 1
    return None


def _extract_config_override(args: Iterable[str], key: str) -> str | None:
    args_list = list(args)
    needle = f"{key}="
    i = 0
    while i < len(args_list):
        token = args_list[i]
        if token in {"-c", "--config"} and i + 1 < len(args_list):
            candidate = args_list[i + 1]
            if candidate.startswith(needle):
                _, raw = candidate.split("=", 1)
                try:
                    return json.loads(raw)
                except json.JSONDecodeError:
                    return raw
        i += 1
    return None


def _ensure_config(proxy_url: str, profile: str) -> str | None:
    (
        provider_key,
        provider_name,
        provider_wire_api,
        provider_env_key,
        model,
        provider_context_window,
    ) = _resolve_provider_config()
    base_url = _format_provider_base_url(provider_key, proxy_url)
    
    effort = os.getenv("OLLAMA_REASONING_EFFORT") or _DEFAULT_REASONING_EFFORT
    check_raw = os.getenv("OLLAMA_REASONING_CHECK")
    if check_raw:
        is_reasoning = check_raw.lower() == "true"
    else:
        is_reasoning = _DEFAULT_IS_REASONING

    lines: list[str] = [
        "# Autogenerated by AstraForge Codex wrapper",
        "",
        f'model = "{model}"',
    ]
    if is_reasoning:
        lines.append(f'model_reasoning_effort = "{effort}"')
    
    lines.extend([
        f'profile = "{profile}"',
        f'model_provider = "{provider_key}"',
        f'sandbox_mode = "danger-full-access"',
        f"[model_providers.{provider_key}]",
        f'name = "{provider_name}"',
        f'base_url = "{base_url}"',
    ])
    if provider_context_window:
        try:
            context_value = int(provider_context_window)
        except ValueError:
            context_value = None
        if context_value is not None:
            lines.append(f"context_window = {context_value}")
    if provider_env_key:
        lines.append(f'env_key = "{provider_env_key}"')
    if provider_wire_api:
        lines.append(f'wire_api = "{provider_wire_api}"')
    lines.extend(
        [
            "",
            f"[profiles.{profile}]",
            'approval_policy = "never"',
            'sandbox_mode = "danger-full-access"',
            f'model = "{model}"',
            'history.persistence = "save-all"',
        ]
    )
    content = "\n".join(lines) + "\n"

    current = None
    if _CONFIG_PATH.exists():
        try:
            current = _CONFIG_PATH.read_text(encoding="utf-8")
        except OSError:
            current = None
    if current == content:
        return provider_env_key
    _CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    _CONFIG_PATH.write_text(content, encoding="utf-8")
    return provider_env_key



def _emit_config_preview() -> None:
    if not _CONFIG_PATH.exists():
        print("[codex-wrapper] config.toml not found after ensure step", flush=True)
        return
    try:
        raw = _CONFIG_PATH.read_text(encoding="utf-8")
    except OSError as exc:
        print(f"[codex-wrapper] failed to read config.toml: {exc}", flush=True)
        return
    print("[codex-wrapper] active config.toml:\n" + raw, flush=True)


def _resolve_real_cli() -> str:
    candidate = os.getenv(_REAL_CLI_ENV, _DEFAULT_REAL_CLI)
    if Path(candidate).is_file():
        return candidate
    # Fall back to PATH lookup for environments that rename differently.
    resolved = shutil.which(candidate) if "/" not in candidate else None
    if resolved:
        return resolved
    print(
        f"[codex-wrapper] Unable to locate real Codex CLI executable at '{candidate}'.",
        file=sys.stderr,
        flush=True,
    )
    sys.exit(127)


def main(argv: Sequence[str] | None = None) -> int:
    if argv is None:
        argv = sys.argv[1:]
    normalized = _normalize_argv(argv)
    rewritten = _convert_spec_flags(normalized)
    default_profile = _resolve_default_profile()
    if not _has_profile_flag(rewritten):
        rewritten = ["--profile", default_profile, *rewritten]
    proxy_override = _extract_proxy_override(rewritten)
    proxy_url = proxy_override or _DEFAULT_PROXY
    provider_env_key = _ensure_config(proxy_url, default_profile)
    _emit_config_preview()
    provider_key, _, _, _, _, _ = _resolve_provider_config()

    real_cli = _resolve_real_cli()
    command_display = shlex.join([real_cli, *rewritten])
    print(
        f"[codex-wrapper] delegating to {real_cli} via: {command_display}", flush=True
    )
    command = [real_cli, *rewritten]
    try:
        child_env = os.environ.copy()
        api_key_override = _extract_config_override(rewritten, "auth.api_key")
        api_key = None
        if isinstance(api_key_override, str) and api_key_override:
            api_key = api_key_override
        elif os.getenv(_DEFAULT_API_KEY_ENV):
            api_key = os.getenv(_DEFAULT_API_KEY_ENV)

        if api_key and provider_env_key and provider_env_key not in child_env:
            child_env[provider_env_key] = api_key
        # Ensure standard OpenAI env vars fall back to the same key for compatibility.
        if api_key and "OPENAI_API_KEY" not in child_env:
            child_env["OPENAI_API_KEY"] = api_key
        normalized_proxy = _normalize_proxy_url(proxy_url)
        if provider_key in {"ollama", "local-llm"}:
            child_env["OLLAMA_HOST"] = normalized_proxy
            child_env["OLLAMA_BASE_URL"] = normalized_proxy
        elif provider_key == "openai":
            child_env["OPENAI_BASE_URL"] = normalized_proxy
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            env=child_env,
        )
    except FileNotFoundError:
        print(
            "[codex-wrapper] Real Codex CLI executable is missing. Ensure `@openai/codex` is installed.",
            file=sys.stderr,
            flush=True,
        )
        return 127

    assert process.stdout is not None
    for line in process.stdout:
        print(f"[codex-real] {line.rstrip()}", flush=True)
    return process.wait()


if __name__ == "__main__":
    sys.exit(main())
